# Поиск максимального элемента вектора

- Студент: Баданов Александр, группа 3823Б1ПР2
- Технология: SEQ, MPI
- Вариант: 3

## 1. Введение
Задача поиска максимального элемента в вектере является фундаментальной операцией в вычислительной математике и анализе данных. Параллельная реализация позволяет значительно ускорить обработку больших массивов данных за счет распределения вычислений между несколькими процессами.

## 2. Постановка задачи
Найти максимальный элемент в целочисленном вектере произвольного размера.

Входные данные: вектор целых чисел vector<int>
Выходные данные: целое число - максимальный элемент
Ограничения: вектор может быть пустым, содержать отрицательные числа, повторяющиеся значения

## 3. Базовый алгоритм (Последовательный)
```cpp
int max_elem = GetInput()[0];

  for (size_t i = 1; i < GetInput().size(); i++) {
    max_elem = std::max(GetInput()[i], max_elem);
  }

  GetOutput() = max_elem;
```
Алгоритм последовательно перебирает все элементы вектора, обновляя значение максимального элемента при нахождении большего значения.

## 4. Схема распараллеливания
### Распределение данных
Исходный вектор делится на равные части между процессами
Первые N процессов получают +1 элемент для балансировки нагрузки
Формула распределения:

```cpp
int base_size = total_elem / world_size;
int remainder = total_elem % world_size;
```

### Коммуникационная схема
1. Процесс 0 (root):
- Рассылает размер данных всем процессам (`MPI_Bcast`)
- Обрабатывает свою часть данных

2. Процессы 1..N-1:
- Получают размер данных (`MPI_Bcast`)
- Находят локальный максимум

3. Глобальная редукция:
- Все процессы участвуют в операции `MPI_Allreduce` с `MPI_MAX`
- Находится глобальный максимум

## 5. Детали реализации
### Структура кода
- `ops_mpi.cpp` - MPI реализация
- `ops_seq.cpp` - SEQ реализация
- `common.hpp` - общие типы данных
- Тесты в папках `tests/functional/` и `tests/performance/`

### Особенности реализации
- Валидация выполняется только на root процессе
- Корректная обработка пустого вектора
- Балансировка нагрузки между процессами

## 6. Экспериментальная установка
### Оборудование и ПО
- **Процессор:** Apple M1
- **ОС:** macOS 15.3.1
- **Компилятор:** clang version 21.1.5
- **Тип сборки:** release
- **MPI:** Open MPI v5.0.8

### Данные для тестирования
- Размеры тестовых данных
- Количество тестовых случаев

## 7. Результаты и обсуждение

### 7.1 Проверка корректности
Корректность проверялась с помощью 12 функциональных тестов, включающих:

- Обычные векторы
- Отрицательные числа
- Граничные случаи ( один элемент)
- Большие векторы (1000 элементов)

### 7.2 Производительность

| Процессы | Время, мс | Ускорение | Эффективность |
|----------|-----------|-----------|---------------|
| 1 (SEQ)  |    0,13   | 1.00      | N/A           |
| 2        |    0,21   | 0,62      | 31%           |
| 4        |    0,41   | 0,32      | 8%            |
| 8        |    1,39   | 0,09      | 1%            |


## 8. Выводы
В ходе работы была успешно решена задача поиска максимального элемента вектора использованием последовательного алгоритма и технологии MPI для параллельных вычислений

- Эффективное распределение данных
- Корректная обработка граничных случаев

### Ограничения
- Накладные расходы MPI для небольших векторов
- Снижение эффективности при большом количестве процессов


## 9. Источники
1. Курс лекций по параллельному программированию Сысоева Александра Владимировича. 
2. Документация по курсу: https://learning-process.github.io/parallel_programming_course/ru

## Приложение

```cpp
bool BadanovAMaxVecElemMPI::RunImpl() {
  const auto& tmp_vec = GetInput();

  int rank = 0;
  int world_size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  int total_elem = 0;
  if (rank == 0) {
    total_elem = static_cast<int>(tmp_vec.size());
  }

  MPI_Bcast(&total_elem, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (total_elem == 0) {
    return true;
  }

  int base_size = total_elem / world_size;
  int remainder = total_elem % world_size;

  int start_i = 0;
  int end_i = 0;

  if (rank < remainder) {
    start_i = rank * (base_size + 1);
    end_i = start_i + (base_size + 1);
  } else {
    start_i = (remainder * (base_size + 1)) + ((rank - remainder) * base_size);
    end_i = std::min(start_i + base_size, total_elem);
  }

  int max_elem_local = INT_MIN;
  for (int i = start_i; i < end_i; i++) {
    max_elem_local = std::max(tmp_vec[i], max_elem_local);
  }

  int max_elem_global = INT_MIN;

  MPI_Allreduce(&max_elem_local, &max_elem_global, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);

  GetOutput() = max_elem_global;
  return true;
}
```